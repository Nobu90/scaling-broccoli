{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMigRmDqkmmh/wVslBNQ4M3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobu90/scaling-broccoli/blob/main/%5BE_06%5Dlyricist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 루브릭\n",
        "\n",
        "## 1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가?\n",
        "\n",
        "특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가?\n",
        "## 2. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
        "\n",
        "텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
        "## 3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
        "\n",
        "텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?"
      ],
      "metadata": {
        "id": "jRmx05WSMoGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "VsTROlbvOuvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s6U5FzGQKbs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69a02bd-b5e9-4417-e5e2-b84877f21cc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "8W1LSpVZlq0w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxMmmC7RKUfC",
        "outputId": "49224d03-f815-44c1-ad30-8c55ed2d7565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os, re\n",
        "import tensorflow as tf\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "hPwIAdKMlwSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file_path = '/content/drive/MyDrive/Exploration/RS2/data/lyrics/*' \n",
        "\n",
        "txt_list = glob.glob(txt_file_path) \n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() \n",
        "        raw_corpus.extend(raw) \n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOfmp0GPNEaW",
        "outputId": "45b45e21-60a8-4ac9-ad5e-56014ac20d44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 189090\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "m4eAmGLcl6o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sentence)\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZES5KfBP7iE",
        "outputId": "4ef5dd12-1ae4-4181-97f6-f4c39222b1f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "\n",
        "    if len(sentence) == 0: continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "corpus[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UthYBO3PQOi6",
        "outputId": "a0ff42de-b7e4-4592-e8f7-7512770a5244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby cant you read the signs i wont bore you with the details baby <end>',\n",
              " '<start> i dont even wanna waste your time <end>',\n",
              " '<start> lets just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i aint mr right but if youre looking for fast love <end>',\n",
              " '<start> if thats love in your eyes <end>',\n",
              " '<start> its more than enough <end>',\n",
              " '<start> had some bad love <end>',\n",
              " '<start> so fast love is all that ive got on my mind ooh ooh <end>',\n",
              " '<start> ooh ooh looking for some affirmation <end>',\n",
              " '<start> made my way into the sun <end>',\n",
              " '<start> my friends got their ladies <end>',\n",
              " '<start> and theyre all having babies <end>',\n",
              " '<start> i just wanna have some fun i wont bore you with the details baby <end>',\n",
              " '<start> i dont even wanna waste your time <end>',\n",
              " '<start> lets just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "_F-YFUNPmhDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        " \n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cuZjcvmPQp7",
        "outputId": "93826952-89bb-4c49-a13b-c25a8bc8448a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 306  23 ...   0   0   0]\n",
            " [  2 215  11 ...   0   0   0]\n",
            " [  2  20  15 ...   0   0   0]\n",
            " ...\n",
            " [  2  22  71 ...   0   0   0]\n",
            " [  2  35  21 ...   0   0   0]\n",
            " [  2  22  71 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x7fbb23ca84c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iSBNTFTg8aQ",
        "outputId": "57ce82d7-9412-4915-a0ac-2a954e8aa954"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  306   23   90 4877    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2  215   11   79  224    4  114    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   20   15 1037 2491    3    0    0    0    0    0    0    0    0\n",
            "     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIQCgH9MgxXn",
        "outputId": "09256c18-b2f6-417b-891b-5c84afc6d65f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : the\n",
            "5 : i\n",
            "6 : you\n",
            "7 : and\n",
            "8 : to\n",
            "9 : a\n",
            "10 : me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1]  \n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])\n",
        "print(src_input.shape)\n",
        "print(tgt_input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSfZe5HJhLoG",
        "outputId": "1796ff94-782e-47c5-ea88-d370f04d68a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  306   23   90 4877    3    0    0    0    0    0    0    0    0]\n",
            "[ 306   23   90 4877    3    0    0    0    0    0    0    0    0    0]\n",
            "(177988, 14)\n",
            "(177988, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train data"
      ],
      "metadata": {
        "id": "TLYIBkqDnCY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=999, shuffle='true')\n",
        "print(enc_train.shape)\n",
        "print(dec_train.shape)\n",
        "print(enc_val.shape)\n",
        "print(dec_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgFIaV5xhiR7",
        "outputId": "ee0dc6a2-22d7-4aa0-b545-c35497c77025"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142390, 14)\n",
            "(142390, 14)\n",
            "(35598, 14)\n",
            "(35598, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "embedding_size = 512\n",
        "hidden_size = 2048\n",
        "\n",
        "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "X06toZHIiKWV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 최근에는 임베딩 사이즈를 512, 768, 1024 등으로 과거보다 크게 주는 경향이 크다고 함\n",
        "\n",
        "### [출처]\n",
        "https://ai.stackexchange.com/questions/28564/how-to-determine-the-embedding-size\n",
        "\n",
        "## 2. 배치 사이즈와, 임베딩 사이즈로는 로스 값이 줄지 않아 히든 사이즈를 2배로 키운 결과 결과가 좋아짐, 레이어의 수를 늘려보았으나 크게 영향을 미치지는 않았음"
      ],
      "metadata": {
        "id": "UvA-zqwhdyrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam() \n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
        "    from_logits=True, \n",
        "    reduction='none'  \n",
        ")\n",
        "\n",
        "lyricist.compile(loss=loss, optimizer=optimizer) "
      ],
      "metadata": {
        "id": "2gzGXTLO5oNr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lyricist.fit(enc_train, dec_train, validation_data = (enc_val, dec_val), epochs = 10, batch_size = 256, validation_batch_size = 128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2s5xNi5ipWC",
        "outputId": "682374da-f21a-4e69-9fb2-f0b6bd71086c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "557/557 [==============================] - 48s 41ms/step - loss: 3.4648 - val_loss: 3.1180\n",
            "Epoch 2/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 2.9394 - val_loss: 2.8494\n",
            "Epoch 3/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 2.6550 - val_loss: 2.6647\n",
            "Epoch 4/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 2.3834 - val_loss: 2.5203\n",
            "Epoch 5/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 2.1216 - val_loss: 2.4078\n",
            "Epoch 6/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 1.8730 - val_loss: 2.3178\n",
            "Epoch 7/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 1.6429 - val_loss: 2.2547\n",
            "Epoch 8/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 1.4400 - val_loss: 2.2102\n",
            "Epoch 9/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 1.2687 - val_loss: 2.1870\n",
            "Epoch 10/10\n",
            "557/557 [==============================] - 22s 39ms/step - loss: 1.1333 - val_loss: 2.1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = lyricist.evaluate(enc_val, dec_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIK5ctp17Jgx",
        "outputId": "f2eb2a1c-9727-4a89-8989-0b710b9ae334"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1113/1113 [==============================] - 8s 7ms/step - loss: 2.1863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 적절한 배치 사이즈는 일반적으로 16 ~ 128로 2의 제곱으로 넣으나 가장 최적의 배치 사이즈란 정해져 있지 않다고 함 \n",
        "## (테스트 결과 여기서는 각 256, 128이 적절하였음, 키울 수록 과적합, 줄일 수록 로스가 커지는 현상이 나타남)\n",
        "\n",
        "### [출처]\n",
        "https://stats.stackexchange.com/questions/308424/how-does-batch-size-affect-adam-optimizer\n",
        "\n",
        "https://deep-learning-study.tistory.com/647"
      ],
      "metadata": {
        "id": "_D_vyDkTchVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Lyrics"
      ],
      "metadata": {
        "id": "pcQomRAom2ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): \n",
        "\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) \n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True: \n",
        "    \n",
        "        predict = model(test_tensor) \n",
        "  \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        " \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated "
      ],
      "metadata": {
        "id": "jdDF1j90hxjL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n"
      ],
      "metadata": {
        "id": "c2QeZfX4h3MA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "75b975e3-ac6e-4060-8a32-8e6807af1358"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you so much i love you so much <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 후기\n",
        "\n",
        "코랩 프로 플러스의 속도를 체감할 수 있었다. 앞으로도 계속 유료 결제를 떠날 수 없을 것 같다.\n",
        "이번 익스는 조건이 좀 명확하게 있어서 정확하게 조건을 맞추느라 까다로웠으나, 실제 공부에는 도움이 많이 되었던 것 같다.\n",
        "덕분에 배치사이즈에 대해서는 정말 많이 검색해본 것 같고, 이번 익스에서 느낀 점은 역시나 단 한 번의 터치, 옵션 변경으로 결과가 드라마틱하게 좋아진다는 건 존재하지 않는 다는 걸 깨달았다. \n",
        "데이터, 그리고 내가 사용하는 모델과 그 안의 옵션, 파라미터들을 더 정확하고 자세히 알고, 각 원리도 총체적으로 이해해야 원하는 결과를 빠르게 얻을 수 있다는 생각을 다시 하게 되었다.\n",
        "그리고 빨리 얻지 못하면, 구글에 계속 돈을 내야 한다는 사실도... \n",
        "그래도 결과도 결과지만 마지막에는 처음에 나왔던 가사들보다 자연스러운 가사가 나와서 기뻤다(후렴구인듯..)\n"
      ],
      "metadata": {
        "id": "cW314ZOCRupG"
      }
    }
  ]
}